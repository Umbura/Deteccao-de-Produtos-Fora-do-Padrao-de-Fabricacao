{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuRgpHoEeQH_"
      },
      "outputs": [],
      "source": [
        "# Célula 1: --- Importações e Preparação do Dataset ---\n",
        "\n",
        "# IMPORTAÇÕES E CONFIGURAÇÃO\n",
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"TensorFlow Versão:\", tf.__version__)\n",
        "\n",
        "# Configurações globais\n",
        "IMG_SIZE = (256, 256)\n",
        "BATCH_SIZE = 16\n",
        "DATASET_PATH = \"content\"\n",
        "CATEGORY = \"bottle\"\n",
        "ARCHIVE_NAME = \"bottle.tar.xz\"\n",
        "\n",
        "# Link do dataset\n",
        "DATASET_URL = \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937370-1629951468/bottle.tar.xz\"\n",
        "archive_path = os.path.join(DATASET_PATH, ARCHIVE_NAME)\n",
        "\n",
        "# DOWNLOAD E EXTRAÇÃO DO DATASET\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    os.makedirs(DATASET_PATH)\n",
        "\n",
        "if not os.path.exists(os.path.join(DATASET_PATH, CATEGORY)):\n",
        "    print(f\"Dataset '{CATEGORY}' não encontrado.\")\n",
        "\n",
        "    if not os.path.exists(archive_path):\n",
        "        print(f\"Arquivo '{ARCHIVE_NAME}' não encontrado. Baixando de {DATASET_URL}...\")\n",
        "        try:\n",
        "            with requests.get(DATASET_URL, stream=True) as r:\n",
        "                r.raise_for_status()\n",
        "                with open(archive_path, 'wb') as f:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "            print(\"Download concluído.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Erro ao baixar o arquivo: {e}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Extraindo arquivos de '{archive_path}' para '{DATASET_PATH}'...\")\n",
        "        with tarfile.open(archive_path, 'r:xz') as tar:\n",
        "            tar.extractall(path=DATASET_PATH)\n",
        "        print(\"Dataset extraído com sucesso.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ocorreu um erro durante a extração: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Dataset '{CATEGORY}' já existe no diretório '{DATASET_PATH}'.\")\n",
        "\n",
        "\n",
        "# DEFINIÇÃO DOS CAMINHOS\n",
        "base_dir = os.path.join(DATASET_PATH, CATEGORY)\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "print(f\"\\nCaminho para dados de treino: {train_dir}\")\n",
        "print(f\"Caminho para dados de teste: {test_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 2: --- Importa a função de pré-processamento específica do ResNet50 ---\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    validation_split=0.2  # 20% dos dados de treino para validação\n",
        ")\n",
        "\n",
        "# Carregador para os dados de TREINO (80% das imagens 'good')\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='input',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Carregador para os dados de VALIDAÇÃO (20% das imagens 'good')\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='input',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Carregador para os dados de TESTE\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=1,\n",
        "    class_mode='input',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"\\nDados prontos para o treinamento, validação e teste.\")"
      ],
      "metadata": {
        "id": "zYd3iE_8egkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 3: --- Definição do Extrator de Features (ResNet50) ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Carregar uma rede pré-treinada ResNet50\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "\n",
        "# A medotodologia do PaDiM usa as saídas dos 3 primeiros blocos.\n",
        "layer_names = ['conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out']\n",
        "outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "\n",
        "feature_extractor = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Congelar o modelo, pois não vamos treiná-lo.\n",
        "feature_extractor.trainable = False\n",
        "\n",
        "print(\"Extrator de features (ResNet50 com 3 blocos) pronto.\")\n",
        "feature_extractor.summary()"
      ],
      "metadata": {
        "id": "KNLkCNXfejA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 4: --- Treinamento: Aprendendo a Distribuição Normal ---\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_generator.reset()\n",
        "\n",
        "feature_map_size = (64, 64)\n",
        "features_list = []\n",
        "\n",
        "print(f\"\\nExtraindo features das {train_generator.n} imagens de treino...\")\n",
        "\n",
        "for i in tqdm(range(len(train_generator))):\n",
        "    x, _ = next(train_generator)\n",
        "\n",
        "    layer1_features, layer2_features, layer3_features = feature_extractor.predict(x, verbose=0)\n",
        "    layer2_resized = tf.image.resize(layer2_features, feature_map_size, method='bilinear')\n",
        "    layer3_resized = tf.image.resize(layer3_features, feature_map_size, method='bilinear')\n",
        "\n",
        "    combined_features = np.concatenate([layer1_features, layer2_resized, layer3_resized], axis=-1)\n",
        "\n",
        "    features_list.append(combined_features)\n",
        "\n",
        "features_stack = np.concatenate(features_list, axis=0)\n",
        "\n",
        "embedding_dims = 100 # Hiperparâmetro\n",
        "total_dims = features_stack.shape[-1]\n",
        "idx = np.random.choice(total_dims, embedding_dims, replace=False)\n",
        "features_reduced = features_stack[:, :, :, idx]\n",
        "\n",
        "print(\"\\nCalculando média e matriz de covariância para cada patch...\")\n",
        "\n",
        "features_reduced_T = features_reduced.transpose(1, 2, 0, 3)\n",
        "\n",
        "mean_embeddings = np.mean(features_reduced_T, axis=2)\n",
        "\n",
        "covariance = np.zeros((feature_map_size[0], feature_map_size[1], embedding_dims, embedding_dims))\n",
        "identity = np.identity(embedding_dims)\n",
        "epsilon = 0.01\n",
        "\n",
        "for h in tqdm(range(feature_map_size[0])):\n",
        "    for w in range(feature_map_size[1]):\n",
        "        features_for_patch = features_reduced_T[h, w, :, :]\n",
        "        cov = np.cov(features_for_patch, rowvar=False) + epsilon * identity\n",
        "        covariance[h, w, :, :] = cov\n",
        "\n",
        "print(\"\\n'Treinamento' concluído. Média e covariância aprendidas.\")"
      ],
      "metadata": {
        "id": "-35N-KahelMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 5: --- Avaliação Geral via AUC-ROC ---\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Cálculo do Score de Anomalia para o Conjunto de Teste\n",
        "test_generator.reset()\n",
        "anomaly_scores = []\n",
        "y_true = (test_generator.classes != test_generator.class_indices['good']).astype(int)\n",
        "\n",
        "# Definir class_names para o relatório de classificação e matriz de confusão\n",
        "class_names = ['good', 'anomaly']\n",
        "\n",
        "print(\"\\nCalculando scores de anomalia para o conjunto de teste...\")\n",
        "\n",
        "if 'anomaly_scores' not in locals() or len(anomaly_scores) != len(y_true):\n",
        "    for i in tqdm(range(len(test_generator))):\n",
        "        x_test, _ = test_generator[i]\n",
        "        # ... (código de extração de features e cálculo da distância de Mahalanobis) ...\n",
        "        # (Este bloco de código interno do loop permanece o mesmo)\n",
        "        layer1_test, layer2_test, layer3_test = feature_extractor.predict(x_test, verbose=0)\n",
        "        layer2_test_resized = tf.image.resize(layer2_test, feature_map_size, method='bilinear')\n",
        "        layer3_test_resized = tf.image.resize(layer3_test, feature_map_size, method='bilinear')\n",
        "        combined_features_test = np.concatenate([layer1_test, layer2_test_resized, layer3_test_resized], axis=-1)\n",
        "        features_test_reduced = combined_features_test[:, :, :, idx]\n",
        "        anomaly_map = np.zeros((feature_map_size[0], feature_map_size[1]))\n",
        "        for h in range(feature_map_size[0]):\n",
        "            for w in range(feature_map_size[1]):\n",
        "                feature_vec = features_test_reduced[0, h, w]\n",
        "                mean_vec = mean_embeddings[h, w]\n",
        "                cov_mat = covariance[h, w]\n",
        "                try:\n",
        "                    L = np.linalg.cholesky(cov_mat)\n",
        "                    y = np.linalg.solve(L, feature_vec - mean_vec)\n",
        "                    dist = np.sqrt(np.dot(y, y))\n",
        "                except np.linalg.LinAlgError:\n",
        "                    dist = mahalanobis(feature_vec, mean_vec, np.linalg.pinv(cov_mat))\n",
        "                anomaly_map[h, w] = dist\n",
        "        anomaly_map_smoothed = gaussian_filter(anomaly_map, sigma=4)\n",
        "        anomaly_scores.append(np.max(anomaly_map_smoothed))\n",
        "    anomaly_scores = np.array(anomaly_scores)\n",
        "    print(\"Cálculo de scores concluído.\")\n",
        "\n",
        "# --- Métrica Independente de Limiar (AUC-ROC) ---\n",
        "fpr, tpr, _ = roc_curve(y_true, anomaly_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(f\"\\nÁrea Sob a Curva ROC (AUC-ROC): {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (área = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsos Positivos')\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
        "plt.title('Curva Característica de Operação do Receptor (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c7WmAiJuU6QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 6: --- Otimização do Limiar e Métricas Finais ---\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_recall_curve, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Calcular o F1-score para cada limiar\n",
        "precision, recall, thresholds = precision_recall_curve(y_true, anomaly_scores)\n",
        "\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "f1_scores = f1_scores[:-1]\n",
        "thresholds = thresholds[:len(f1_scores)]\n",
        "\n",
        "# Encontrar o melhor F1-score e o limiar correspondente\n",
        "best_f1_idx = np.argmax(f1_scores)\n",
        "best_f1 = f1_scores[best_f1_idx]\n",
        "best_threshold = thresholds[best_f1_idx]\n",
        "\n",
        "print(f\"Melhor F1-Score encontrado: {best_f1:.4f}\")\n",
        "print(f\"Limiar correspondente: {best_threshold:.4f}\")\n",
        "\n",
        "# --- REAVALIAR COM O LIMIAR OTIMIZADO ---\n",
        "y_pred_best = (anomaly_scores > best_threshold).astype(int)\n",
        "\n",
        "print(\"\\n--- Métricas Baseadas no Limiar Otimizado ---\\n\")\n",
        "print(\"Relatório de Classificação Otimizado:\\n\")\n",
        "print(classification_report(y_true, y_pred_best, target_names=class_names))\n",
        "\n",
        "print(\"\\nMatriz de Confusão Otimizada:\")\n",
        "cm_best = confusion_matrix(y_true, y_pred_best)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predito')\n",
        "plt.ylabel('Verdadeiro')\n",
        "plt.title('Matriz de Confusão com Limiar Otimizado')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2maTe9_FdQ7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 7: --- Visualização dos Mapas de Anomalia ---\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def visualize_padim_results(test_generator, feature_extractor, mean_embeddings, covariance, idx, num_good=3, num_anomaly=3):\n",
        "    \"\"\"\n",
        "    Função para visualizar a imagem original de teste e seu mapa de anomalia,\n",
        "    mostrando um número específico de amostras 'good' e de anomalia.\n",
        "    \"\"\"\n",
        "    test_generator.reset()\n",
        "    filenames = test_generator.filenames\n",
        "    labels = test_generator.classes\n",
        "    class_indices = test_generator.class_indices\n",
        "\n",
        "    # Encontrar os índices para cada tipo de classe\n",
        "    good_indices = np.where(labels == class_indices['good'])[0]\n",
        "    anomaly_indices = np.where(labels != class_indices['good'])[0]\n",
        "\n",
        "    # Garantir que não vamos pedir mais amostras do que as disponíveis\n",
        "    num_good_to_show = min(num_good, len(good_indices))\n",
        "    num_anomaly_to_show = min(num_anomaly, len(anomaly_indices))\n",
        "\n",
        "    # Selecionar amostras aleatórias de cada classe (sem reposição)\n",
        "    selected_good = np.random.choice(good_indices, num_good_to_show, replace=False)\n",
        "    selected_anomaly = np.random.choice(anomaly_indices, num_anomaly_to_show, replace=False)\n",
        "\n",
        "    # Combinar os índices para visualização\n",
        "    indices_to_show = np.concatenate([selected_good, selected_anomaly])\n",
        "\n",
        "    print(f\"\\nVisualizando {len(indices_to_show)} mapas de anomalia ({num_good_to_show} da classe 'good' e {num_anomaly_to_show} de anomalias):\\n\")\n",
        "\n",
        "    for i in indices_to_show:\n",
        "        x_test, _ = test_generator[i]\n",
        "        label = os.path.dirname(filenames[i])\n",
        "\n",
        "        # Extração de features\n",
        "        layer1_test, layer2_test, layer3_test = feature_extractor.predict(x_test, verbose=0)\n",
        "        layer2_resized = tf.image.resize(layer2_test, feature_map_size, method='bilinear')\n",
        "        layer3_resized = tf.image.resize(layer3_test, feature_map_size, method='bilinear')\n",
        "        combined_features = np.concatenate([layer1_test, layer2_resized, layer3_resized], axis=-1)\n",
        "        features_test_reduced = combined_features[:, :, :, idx]\n",
        "\n",
        "        # Cálculo do mapa de anomalia\n",
        "        anomaly_map = np.zeros(feature_map_size)\n",
        "        for h in range(feature_map_size[0]):\n",
        "            for w in range(feature_map_size[1]):\n",
        "                feature_vec = features_test_reduced[0, h, w]\n",
        "                mean_vec = mean_embeddings[h, w]\n",
        "                cov_mat = covariance[h, w]\n",
        "                try:\n",
        "                    L = np.linalg.cholesky(cov_mat)\n",
        "                    y = np.linalg.solve(L, feature_vec - mean_vec)\n",
        "                    dist = np.sqrt(np.dot(y, y))\n",
        "                except np.linalg.LinAlgError:\n",
        "                    dist = mahalanobis(feature_vec, mean_vec, np.linalg.pinv(cov_mat))\n",
        "                anomaly_map[h, w] = dist\n",
        "\n",
        "        # Redimensiona e suaviza o mapa\n",
        "        anomaly_map_resized = cv2.resize(anomaly_map, IMG_SIZE, interpolation=cv2.INTER_LINEAR)\n",
        "        anomaly_map_smoothed = gaussian_filter(anomaly_map_resized, sigma=4)\n",
        "\n",
        "        # Plot\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        fig.suptitle(f\"Amostra: {filenames[i]} (Classe Real: {label})\", fontsize=16)\n",
        "\n",
        "        # Imagem Original\n",
        "        img_to_show = x_test[0].copy()\n",
        "        img_to_show[:, :, 0] += 103.939\n",
        "        img_to_show[:, :, 1] += 116.779\n",
        "        img_to_show[:, :, 2] += 123.68\n",
        "        img_to_show = img_to_show[:, :, ::-1] # BGR -> RGB\n",
        "        img_to_show = np.clip(img_to_show, 0, 255).astype('uint8')\n",
        "\n",
        "        axes[0].imshow(img_to_show)\n",
        "        axes[0].set_title('Imagem Original')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        im = axes[1].imshow(anomaly_map_smoothed, cmap='jet')\n",
        "        axes[1].set_title('Mapa de Calor da Anomalia')\n",
        "        axes[1].axis('off')\n",
        "\n",
        "        axes[2].imshow(img_to_show)\n",
        "        axes[2].imshow(anomaly_map_smoothed, cmap='jet', alpha=0.5)\n",
        "        axes[2].set_title('Sobreposição')\n",
        "        axes[2].axis('off')\n",
        "\n",
        "        plt.colorbar(im, ax=axes, orientation='horizontal', pad=0.05, fraction=0.05)\n",
        "        plt.show()\n",
        "\n",
        "# Vizualizar 3 amostras 'good' e 3 amostras com anomalia\n",
        "visualize_padim_results(test_generator, feature_extractor, mean_embeddings, covariance, idx, num_good=3, num_anomaly=3)"
      ],
      "metadata": {
        "id": "rzz2bWH5Ng9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
